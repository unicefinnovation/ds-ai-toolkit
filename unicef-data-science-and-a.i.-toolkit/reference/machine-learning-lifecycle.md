# Machine Learning lifecycle

The ultimate aim of a machine learning model is to **generalise**; to learn from past data and be able to provide some insight on new data typically provided by an end user. Machine learning models are made available to end users by putting the model into **production**. This might look slightly different depending on the end user; some models might be for internal use and so the end users are your colleagues who only require an API or your end users might be paying customers who want a mobile app that exposes the output of your model to them. Whatever your model and use case look like, if your ML model is going to be used then it should follow the **Machine Learning Lifecycle**.



<figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcXZmaUTYyojvjW8JLxG_ZeEalmWr0VXG9QLrSSUHS2JSNn-k6c8HNZhjVuMIZPeX5WO-ynTmNe_l9T4bO1N-msp3mM6uSt0Hu-wbxIb8yFWXC73HqmZFcNdahWeb3WlroYceOQHXNwsN_qIT0J4hdENqs?key=x7y24dvBIc283ZMkSlUx_Q" alt="Machine learning lifecycle"><figcaption><p>Reproduced from <a href="https://medium.com/@sabihaali1/machine-learning-lifecycle-80c4a265e589">https://medium.com/@sabihaali1/machine-learning-lifecycle-80c4a265e589</a></p></figcaption></figure>

It is challenging to take an ML model and put it into production \[[StackOverflow: Putting machine learning models into production](https://stackoverflow.blog/2020/10/12/how-to-put-machine-learning-models-into-production/)]. In fact it has been reported that only a small minority of models end up being deployed. Part of the reason for this is that the knowledge and tools to do this, referred to as ML Ops, differ from those needed by a data scientist \[[Blog: Why data scientists don’t need to know Kubernetes](https://huyenchip.com/2021/09/13/data-science-infrastructure.html)].

The important piece to notice is that this is an iterative cycle: ML models should not simply be trained once, then deployed and then considered ‘completed’. All ML models will change their behaviour over time as the underlying data changes. It is important to monitor these changes and periodically retrain and update the model and re-deploy (this is known as **Machine Learning Operations or MLOps**) .

### Scoping

Machine learning projects typically start with **scoping or defining** the project. This includes deciding on the objectives of the project and what makes it a good candidate for a machine learning application. Some questions to ask include

* Why is the problem important?
* Who does the problem affect?
* What if we don’t have the right data to solve the problem?
* When is the project over?
* What if we don’t like the results?

This is about identifying the end users. Identify the people who will be affected by the problem and incorporate them into the discussion of problem articulation. This can mean small group or focus group discussions if they are representative of a larger group of people. Understand how these people will be affected by the project. One approach to think about this is to have a solution trial run. Assuming you can answer the problem, ask yourself if you can use the answer and whose life will change as a result. The [Product Design mentoring stream](https://app.gitbook.com/o/dzsBDD9SiWXEEyjPOOwx/s/yz1WKISRzccLblgdJi7L/) covers this as does the [Diversity, Equity and Inclusion mentoring stream](https://app.gitbook.com/o/dzsBDD9SiWXEEyjPOOwx/s/yug8NhDpAqWDsAiIVA9F/).

### Collecting Data

**Data science**  relies on collecting data from some source. Typically, data is generated by human activity, such as logs of activity on a web platform (for e.g., clicks, score records, etc.), recorded events (for e.g., visits to medical facilities, attendance at school, etc.), text scripts, visual images or audio recordings. Most data generated in this way is _observational_ whereby the data collector has no control over the data generating process that occurs in the real world. Observational data is collected based on what is seen or heard by people or a computer passively observing some process.

Data can also be experimental, whereby data is collected in a controlled environment following a scientific method. **Experimental data** is not passively collected, but rather it is collected methodically to answer a specific question typically in a controlled setting. In experimental settings, a group of people or things are randomly assigned to treatment and control groups. For example, in drug trials, a treatment group is given some dosage of a drug, while a control group would be assigned a placebo. Experiments lend themselves best to cause-and-effect studies because assigned to treatment and control are randomized and therefore, latent confounding factors that might differentiate the groups can be controlled or explicitly identified. In contrast, cause-and-effect claims cannot be made readily with observational data unless all explicit and latent factors can be controlled for and there is some source of external source of variation that can be attributed directly to the effect (a challenging analytical task). Controlled experiments of this type are often used in A/B testing and user design testing.

Another consideration is whether the data is **structured** or **unstructured**. Structured data has a sense of order and is typically in a row-column structured framework such as spreadsheets or database tables. Unstructured data can be images, text scripts or audio files. Unstructured data can require additional processing to convert their attributes into structured data format for analysis methods. Structured data is usually stored in CSV files, relational databases, Excel files etc whereas unstructured data usually lives in JSON files, image or video files.

### Exploring and Quality Checking

There are a few dimensions to data quality \[[Precisely Data Quality Dimensions](https://www.precisely.com/blog/data-quality/data-quality-dimensions-measure)]

* **Accuracy:** How well does the data reflect reality?
* **Completeness:** Is the data comprehensive or missing value entries in unexpected instances?
* **Consistency:** Does information stored in one place match the information stored in another place?
* **Timeliness:** Is data available when needed?
* **Validity:** Is the data in a specific format? Is it an unusable format? Does it follow business rules?
* **Uniqueness:** Is the data instance the only instance in which the data appears in the sample?

### Data Processing and Feature Engineering

**Data pre-processing** has to do with the process of managing, analyzing, filtering, transforming, encoding and preparing data to be usefully processed by the machine. This might be filling in missing values or normalising different spellings of the same name.

This is typically one of the most time-consuming aspects of the data science process. Any decisions made by the data scientist in the data pre-processing stage can have important impacts in subsequent model development and, even, deployment stages. Therefore, it is important to document analytical decisions made in this stage, whether in the code or through a documenting tool, to trace the lineage of the data from the original data source, when the data is in its raw form, to the model training and development stage.

**Feature Engineering** is specifically preparing your data to be input into a machine learning model. Classical ML models need more feature engineering whereas more modern and complex models are able to accurately model data without so much feature engineering (provided there is sufficient data).&#x20;

Common Feature engineering includes

* **One-hot encoding categorical data**: if car has a column to describe the colour with possible values 'red'/ 'green'/ 'blue' this should be converted to 3 binary columns only one of which will be '1'.
* **Log scaling skewed features**: it is very common that data has a few very large values very far from the mean of the other values. This can be difficult to model, by log scaling, the data will look closer to a normal distribution which corresponds to assumptions made by many models.

### Model Development & Testing

Then, it is defining the data and establishing a baseline. For example, if we can accurately predict an outcome 50% of the time just by flipping a coin, a model should, at least, do better than that. Features should be engineered attuned to the underlying characteristics of the data (for instance, different treatment of categorical versus numerical data types) and business-specific features related to the defined problem. Often, data exploration and data mining inform the data scientist about the features to craft. Moreover, we should ensure the data is labeled consistently in the case of a supervised learning problem.

The modeling process involves selecting an algorithm or model framework (what algorithmic framework makes sense for the problem and data at hand) and training the model to learn the representations in the input data that get us closer to the output data. Error analysis is performed to evaluate and improve the model. Typically, the model that best minimizes the error is selected for deployment. Often, modeling informs us about issues in the data organization and labeling, leading us to go back and fix issues in the data.

Deployment is typically the final step in the process. Once the model is selected, and it is put into production, it should be continually monitored through error analysis and the entire system should be maintained to detect any changes in the underlying data distribution serving as inputs to the model. Over time, it is common for machine learning models to require maintenance and retraining (reverting back to model development and then re-deployed).

### Model Deployment

A trained model is typically deployed on a cloud computing resource. The model will perform **inference** on some data and produce a result; a prediction, categorisation, generation or other. Thsi might be exposed through an app or an API. This is the point at which your technology interacts directly with the user. You should think carefully about uer expectations at this point e.g. is the latency acceptable? Does the user understand that this is a prediction? What happens if the user inputs badly formatted data? These questions are particularly important in LLM applications.

Monitoring shouod be put in place once a model is deployed to see if the accuracy is maintained or if other important metrics are stable.

See also:

* [Coursera: Machine learning in production](https://www.coursera.org/learn/introduction-to-machine-learning-in-production)
